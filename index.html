
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en">
<head>
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-108114467-1', 'auto');
        ga('send', 'pageview');
    </script>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="chenxu hu,thu,Chenxu Hu,chenxuhu,hu chenxu,Hu Chenxu,huchenxu,THU,tsinghua,ZJU,zju">
    <meta name="description" content="">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <link href='https://fonts.googleapis.com/css?family=Titillium Web' rel='stylesheet'>
    <meta charset="utf-8">
    <style type="text/css">
        /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus, a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body, td, th {
            font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: 400;
        }
        
        heading {
            font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
            font-size: 19px;
            font-weight: bold
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: bold;
        }

        strongred {
            font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
            color: 'red';
            font-size: 16px
        }

        sectionheading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 22px;
            font-weight: bold
        }

        tableheading {
            font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
            font-size: 16px;
        }

        .circle{
            width: 240px; 
            height: 240px; 
            border-radius: 50%; 
            overflow: hidden;
        }

        .circle > img{
            width: 100%;
            height: 100%;
        }
    </style>
    <link rel="shortcut icon" type="image/png" href="images/avatar.jpg"/>
    <title>Chenxu Hu - THU</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
          type='text/css'>
    <script async defer src="https://buttons.github.io/buttons.js"></script>
</head>
<body>
<table width="900" border="0" align="center" cellspacing="0" cellpadding="20">
    <tr>
        <td halign="center">
            <p align="center">
                <font size="6">Chenxu Hu&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;胡晨旭</font>
            </p>
        </td>
    </tr>
    <tr>
        <td>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="67%" valign="middle" style="text-align:justify">
                        <p>Hey, I am Chenxu Hu, currently a Ph.D. student in Computer Science at 
                            <a href="https://iiis.tsinghua.edu.cn/en/">IIIS</a>, <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>
                            advised by <a href="https://hangzhaomit.github.io/">Prof. Hang Zhao</a>. I am
                            also a research assistant in
                            <a href="https://group.iiis.tsinghua.edu.cn/~marslab/">MARS Lab</a>
                            at Tsinghua University. 
                        </p>
                        <p>    
                            I am especially interested in
                            multi-modal machine learning and audio & speech processing, 
                            including speech synthesis, audio-visual learning, and some novel tasks 
                            combining audio, vision, language and other modalities. 
                            My research vision is to enable machines to learn, reason and interact from multi-modal inputs, 
                            just like human beings.
                        </p>
                        <p>Previously, I received my B.E. in Computer Science from <a href="http://ckc.zju.edu.cn/ckcen/main.htm">Chu Kochen Honors College</a>, 
                        <a href="https://www.zju.edu.cn/english/">Zhejiang University</a>. 
        
                        <p align="center">
                            <!-- <a href="files/cv.pdf">CV</a> | -->
                            <a href="mailto:chenxuhu65@gmail.com">E-Mail</a> | 
                            <a href="https://scholar.google.com/citations?user=4LzKZggAAAAJ">Google Scholar</a> |
                            <a href="https://github.com/huchenxucs">Github</a> 
                        </p>
                    </td>
                    <td width="100%" valign="top">
                    <div class="circle">
                        <img src="images/avatar.jpg" width="100%">
                    </div>    
                    </td>
                </tr>
            </table>

            <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td>
                        <sectionheading>News</sectionheading>
                    </td>
                </tr>
            </table>
            <table class="news-table" width="100%" align="center" border="0" style="text-align: justify">
                <colgroup>
                    <col width="15%">
                    <col width="85%">
                </colgroup>
                <tbody>
                <tr>
                    <td valign="top" align="center"><tableheading>Jan. 2022</tableheading></td>
                    <td>
                        An invited <a href="https://www.techbeat.net/talk-info?id=629">talk</a> at <a href="https://www.techbeat.net/">JiangMen TechBeat</a>, about Neural Dubber: Dubbing for Videos According to Scripts
                    </td>
                </tr>
                <tr>
                    <td valign="top" align="center"><tableheading>Oct. 2021</tableheading></td>
                    <td>One paper accepted to NeurIPS 2021!
                    </td>
                </tr>
                </tbody>
            </table> -->

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="100%" valign="middle">
                        <sectionheading>Publications</sectionheading>
                        <p style="font-size: 20px"> * indicates equal contribution</p>
                    </td>
                </tr>
            </table>


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >

                <tr>
                    <td width="40%">
                        <div class="one">
                            <div class="two"><img src='images/chatdb.png' alt="sym" width="100%"
                                                            style="border-style: none"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <p><a href="https://chatdatabase.github.io/">
                            <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none">
                            <heading>ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory</heading>
                        </a><br>
                            <strong>Chenxu Hu*</strong>, Jie Fu*, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao<br>
                            <em>LLM @ IJCAI 2023<br></em>
                            <a href="https://arxiv.org/pdf/2306.03901.pdf">paper</a> |
                            <a href="https://chatdatabase.github.io/">project</a> |
                            <a href="https://github.com/huchenxucs/ChatDB/">code</a>
                    </td>
                </tr>

                <tr>
                    <td width="40%">
                        <div class="one">
                            <div class="two"><img src='images/diff-foley.png' alt="sym" width="100%"
                                                            style="border-style: none"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <p><a href="https://diff-foley.github.io/">
                            <!-- <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none"> -->
                            <heading>DIFF-FOLEY: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models</heading>
                        </a><br>
                            Simian Luo, Chuanhao Yan, <strong>Chenxu Hu</strong>, Hang Zhao<br>
                            <em>NeurIPS 2023<br></em>
                            <a href="https://arxiv.org/pdf/2306.17203.pdf">paper</a> |
                            <a href="https://diff-foley.github.io/">project</a> |
                            <a href="https://github.com/luosiallen/Diff-Foley/">code</a>
                    </td>
                </tr>

                <tr>
                    <td width="40%">
                        <div class="one">
                            <div class="two"><img src='images/diclet-tts.png' alt="sym" width="100%"
                                                            style="border-style: none"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <p><a href="https://silyfox.github.io/DiCLETdemo/">
                            <!-- <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none"> -->
                            <heading>DiCLET-TTS: Diffusion Model based Cross-lingual Emotion Transfer for Text-to-Speech — A Study between English and Mandarin</heading>
                        </a><br>
                            Tao Li, <strong>Chenxu Hu</strong>, Jian Cong, Xinfa Zhu, Jingbei Li, Qiao Tian, Yuping Wang, Lei Xie<br>
                            <em>Transactions on Audio, Speech, and Language Processing (TASLP)<br></em>
                            <a href="https://arxiv.org/pdf/2309.00883.pdf">paper</a> |
                            <a href="https://silyfox.github.io/DiCLETdemo/">project</a>
                    </td>
                </tr>

                <tr>
                    <td width="40%">
                        <div class="one">
                            <div class="two"><img src='images/vip3d.png' alt="sym" width="100%"
                                                            style="border-style: none"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <p><a href="https://tsinghua-mars-lab.github.io/ViP3D/">
                            <!-- <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none"> -->
                            <heading>ViP3D: End-to-end Visual Trajectory Prediction via 3D Agent Queries</heading>
                        </a><br>
                            Junru Gu*, <strong>Chenxu Hu*</strong>, Tianyuan Zhang, Xuanyao Chen, Yilun Wang, Yue Wang, Hang Zhao<br>
                            <em>CVPR 2023<br></em>
                            <a href="https://arxiv.org/pdf/2208.01582.pdf">paper</a> |
                            <a href="https://tsinghua-mars-lab.github.io/ViP3D/">project</a> |
                            <a href="https://github.com/Tsinghua-MARS-Lab/ViP3D">code</a>
                    </td>
                </tr>

                <tr>
                    <td width="40%">
                        <div class="one">
                            <div class="two"><img src='images/clone.png' alt="sym" width="100%"
                                                            style="border-style: none"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <p><a href="https://xcmyz.github.io/CLONE/">
                            <!-- <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none"> -->
                            <heading>Controllable and Lossless Non-Autoregressive End-to-End Text-to-Speech</heading>
                        </a><br>
                            Zhengxi Liu*, Qiao Tian*, <strong>Chenxu Hu*</strong>, Xudong Liu, Menglin Wu, Yuping Wang, Hang Zhao, Yuxuan Wang<br>
                            <!-- <em>NeurIPS 2021<br></em> -->
                            <a href="https://arxiv.org/pdf/2207.06088.pdf">paper</a> |
                            <a href="https://xcmyz.github.io/CLONE/">project</a>
                    </td>
                </tr>



                <tr>
                    <td width="40%">
                        <div class="one">
                            <div class="two"><img src='images/neuraldubber.png' alt="sym" width="100%"
                                                            style="border-style: none"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <p><a href="https://tsinghua-mars-lab.github.io/NeuralDubber/">
                            <!-- <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none"> -->
                            <heading>Neural Dubber: Dubbing for Videos According to Scripts</heading>
                        </a><br>
                            <strong>Chenxu Hu</strong>, Qiao Tian, Tingle Li, Yuping Wang, Yuxuan Wang, Hang Zhao<br>
                            <em>NeurIPS 2021<br></em>
                            <a href="https://arxiv.org/abs/2110.08243">paper</a> |
                            <a href="https://tsinghua-mars-lab.github.io/NeuralDubber/">project</a>
                    </td>
                </tr>
                
                <tr>
                    <td width="40%">
                        <div class="one">
                            <div class="two"><img src='images/cvc.jpg' alt="sym" width="100%"
                                                              style="border-style: none"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <p><a href="https://tinglok.netlify.app/files/cvc/">
                            <!-- <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none"> -->
                            <heading>CVC: Contrastive Learning for Non-parallel Voice Conversion</heading>
                        </a><br>
                            Tingle Li*, Yichen Liu*, <strong>Chenxu Hu*</strong>, Hang Zhao<br>
                            <em>Interspeech 2021<br></em>
                            <a href="https://arxiv.org/abs/2011.00782">paper</a> |
                            <a href="https://tinglok.netlify.app/files/cvc/">project</a> |
                            <a href="https://github.com/Tinglok/CVC">code</a>
                    </td>
                </tr>

                <tr>
                    <td width="40%">
                        <div class="one">
                            <div class="two"><img src='images/fs2.png' alt="sym" width="100%"
                                                                  style="border-style: none"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <p><a href="https://speechresearch.github.io/fastspeech2/">
                            <!-- <img src="images/new.png" alt="[NEW]" width="5%" style="border-style: none"> -->
                            <heading>FastSpeech 2: Fast and High-Quality End-to-End Text to Speech</heading>
                        </a><br>
                            Yi Ren*, <strong>Chenxu Hu*</strong>, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu<br>
                            <em>ICLR 2021<br></em>
                            <a href="https://arxiv.org/abs/2006.04558">paper</a> |
                            <a href="https://speechresearch.github.io/fastspeech2/">project</a> |
                            <a href="https://github.com/espnet/espnet/tree/master/espnet2/tts/fastspeech2">code</a>
                    </td>
                </tr>

            </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td>
                            <sectionheading>Presentations & Talks</sectionheading>
                        </td>
                    </tr>
                </table>
                <!-- 一月Jan，二月Feb，三月Mar，四月Apr，五月May，六月Jun，七月Jul，八月Aug，九月Sept，十月Oct，十一月Nov，十二月Dec。   -->
                <table class="news-table" width="100%" align="center" border="0" style="text-align: justify">
                    <colgroup>
                        <col width="15%">
                        <col width="85%">
                    </colgroup>
                    <tbody>
                    
                    <tr>
                        <td valign="top" align="center"><tableheading>Aug. 2023</tableheading></td>
                        <td> Invited talk at <a href="https://www.techbeat.net/">JiangMen TechBeat</a>, 
                            "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory" 
                            (<a href="https://www.techbeat.net/talk-info?id=805">Talk</a>)
                        </td>
                    </tr>

                    <tr>
                        <td valign="top" align="center"><tableheading>Aug. 2023</tableheading></td>
                        <td> Invited paper talk at <a href="https://bigmodel.ai/llm-ijcai23">Symposium on Large Language Models (LLM 2023)</a>, 
                            "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory"
                        </td>
                    </tr>
                    
                    <tr>
                        <td valign="top" align="center"><tableheading>Jan. 2022</tableheading></td>
                        <td> Invited talk at <a href="https://www.techbeat.net/">JiangMen TechBeat</a>, 
                            "Neural Dubber: Dubbing for Videos According to Scripts" 
                            (<a href="https://www.techbeat.net/talk-info?id=629">Talk</a>)
                        </td>
                    </tr>

                    <tr>
                        <td valign="top" align="center"><tableheading>June 2021</tableheading></td>
                        <td> Invited paper talk at <a href="https://sightsound.org/">Sight and Sound Workshop</a>, CVPR 2021, 
                            "Neural Dubber: Dubbing for Silent Videos According to Scripts" 
                        (<a href="https://youtu.be/IEFuj7WGO-c?t=2899">Talk</a>)
                        </td>
                    </tr>

                    <tr>
                        <td valign="top" align="center"><tableheading>May 2021</tableheading></td>
                        <td> Poster presentation at <a href="https://iclr.cc/Conferences/2021">ICLR 2021</a>, 
                            "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech"
                        </td>
                    </tr>

                    </tbody>
                </table>


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td>
                        <sectionheading>Media Coverage</sectionheading>
                    </td>
                </tr>
            </table>
            <table class="news-table" width="94%" align="center" border="0" style="text-align: justify">
                <colgroup>
                    <col width="25%">
                    <col width="75%">
                </colgroup>
                <tbody>
                <tr>
                    <td valign="top" align="left"><tableheading>Jiqizhixin(机器之心) Blog</tableheading></td>
                    <td><a href="https://mp.weixin.qq.com/s/o3j1vNLHlJ6qTea219A4Qw">
                        A Chinese news about ChatDB</a>
                    </td>
                </tr>
                <tr>
                    <td valign="top" align="left"><tableheading>Slator</tableheading></td>
                    <td><a href="https://slator.com/neural-dubber-tiktok-company-bytedance-explores-automated-dubbing/">
                        Neural Dubber: TikTok Parent Company ByteDance Explores Automated Dubbing</a>
                    </td>
                </tr>
                <tr>
                    <td valign="top" align="left"><tableheading>Jiqizhixin(机器之心) Blog</tableheading></td>
                    <td><a href="https://mp.weixin.qq.com/s/I4BNwN25JOJoTs8stzDqUg">
                        A Chinese news about Neural Dubber</a>
                    </td>
                </tr>
                <tr>
                    <td valign="top" align="left"><tableheading>Microsoft Research Blog</tableheading></td>
                    <td><a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/articles/fastspeech-2-fast-and-high-quality-end-to-end-text-to-speech/">
                        FastSpeech 2: Fast and High-Quality End-to-End Text to Speech</a>
                    </td>
                </tr>
                <tr>
                    <td valign="top" align="left"><tableheading>Microsoft Azure AI Blog</tableheading></td>
                    <td><a href="https://techcommunity.microsoft.com/t5/azure-ai-blog/neural-text-to-speech-extends-support-to-15-more-languages-with/ba-p/1505911?from=timeline">
                        Neural Text to Speech extends support to 15 more languages with state-of-the-art AI quality</a>
                    </td>
                </tr>
                </tbody>
            </table>


                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tbody>
                    <tr>
                        <td>
                            <sectionheading>Affiliations</sectionheading>
                        </td>
                    </tr>
                    </tbody>
                </table>
                <table align="center">
                    <tbody>
                    <tr>
                        <td width="28%" align="center">
                            <a href="https://www.zju.edu.cn/english/" target="_blank">
                                <img style="width:120px" src="images/zju.png"></a>&nbsp &nbsp
                        </td>
                        <td width="28%" align="center">
                            <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/" target="_blank">
                                <img style="width:120px" src="images/msra.png"></a>&nbsp &nbsp
                        </td>
                        <td width="28%" align="center">
                            <a href="https://www.tsinghua.edu.cn/en/" target="_blank">
                                <img style="width:120px" src="images/thu.png"></a>&nbsp &nbsp
                        </td>
                        <td width="28%" align="center">
                            <a href="https://www.bytedance.com/en/" target="_blank">
                                <img style="width:120px" src="images/bytedance.png"></a>&nbsp &nbsp
                        </td>
                    </tr>
                    <tr>
                        <td width="28%" align="center"><font size="3">ZJU<br>2017-2021</font></td>
                        <td width="28%" align="center"><font size="3">MSRA<br>2019-2020</font></td>
                        <td width="28%" align="center"><font size="3">THU<br>2021 - present</font></td>
                        <td width="28%" align="center"><font size="3">ByteDance<br>2021 - 2022</font></td>
                    </tr>
                    </tbody>
                </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td><br>
                        <p align="right"><font size="1">
                            Template credits: <a href="https://changan.io/">Changan Chen</a>
                        </font></p></td>
                </tr>
                </tbody>
            </table>

        </td>
    </tr>
</table>
</body>
</html>
